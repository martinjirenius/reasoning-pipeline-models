{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b325bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Sampler(object):\n",
    "    r\"\"\"Base class for all Samplers.\n",
    "\n",
    "    Every Sampler subclass has to provide an __iter__ method, providing a way\n",
    "    to iterate over indices of dataset elements, and a __len__ method that\n",
    "    returns the length of the returned iterators.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source):\n",
    "        pass\n",
    "\n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class WeightedRandomSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, labels, stratify = None, weights = None):\n",
    "        self.labels = labels\n",
    "        self.label_counts = labels.value_counts().to_dict()\n",
    "        self.num_labels = len(self.label_counts.keys())\n",
    "        \n",
    "        if weights is None:\n",
    "            self.weights = {key: 1 for key, val in self.label_counts.items()}\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        if stratify == 'increase':\n",
    "            self.samples_per_label = {key: round(max(self.label_counts.values()) * weight) for key, weight in self.weights.items()}\n",
    "        elif stratify == 'decrease':\n",
    "            self.samples_per_label = {key: round(min(self.label_counts.values()) * weight) for key, weight in self.weights.items()}\n",
    "        else:\n",
    "            self.samples_per_label = {key: round(self.label_counts[key] * self.weights[key]) for key in self.weights} \n",
    "        \n",
    "        self.num_samples = sum(self.samples_per_label.values())\n",
    "        \n",
    "    def __iter__(self):\n",
    "        indices = []\n",
    "        for lbl, amount in self.samples_per_label.items():\n",
    "            label_data = self.labels[self.labels == lbl]\n",
    "            label_counts = len(label_data)\n",
    "            for i in range(amount // label_counts):\n",
    "                indices += label_data.index[torch.randperm(label_counts)].tolist()\n",
    "            indices += label_data.index[torch.randperm(amount % label_counts)].tolist()\n",
    "        return iter(np.array(indices)[torch.randperm(self.num_samples)].tolist())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e799e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class LM_Datasets(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_paths, split = [0,1]):\n",
    "        \n",
    "        self.encodings = pd.DataFrame()\n",
    "        self.others = pd.DataFrame(columns = ['cls_labels'])\n",
    "        for (data_path, data_id) in data_paths:\n",
    "            dataset = pd.read_csv(f'{data_path}', header = 0, index_col = 0)\n",
    "            dataset = dataset[round(len(dataset)*split[0]): round(len(dataset)*split[1])]\n",
    "            dataset['data_id'] = data_id\n",
    "            lit_evals = ['input_ids', 'attention_mask', 'gold_ids', 'gold_mask']\n",
    "    \n",
    "            tqdm.pandas()        \n",
    "            dataset[lit_evals] = dataset[lit_evals].progress_applymap(literal_eval)\n",
    "            \n",
    "            num_labels = len(pd.unique(self.others['cls_labels']))\n",
    "            \n",
    "            lbls = np.sort(pd.unique(dataset['labels']))\n",
    "            \n",
    "            dataset['cls_labels'] = dataset['labels'].apply(lambda lbl: np.where(lbls == lbl)[0][0] + num_labels)\n",
    "        \n",
    "            self.encodings = self.encodings.append(dataset[lit_evals])\n",
    "            self.others = self.others.append(dataset.drop(lit_evals, axis=1))\n",
    "        \n",
    "            lm_labels = lambda x: [-100]*len(x['input_ids']) + x['gold_ids'][(len(x['input_ids'])):]\n",
    "            self.encodings['lm_labels'] = self.encodings.apply(lm_labels, axis=1)\n",
    "        self.encodings = self.encodings.reset_index()\n",
    "        self.others = self.others.reset_index()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encodings.loc[idx].to_dict('list'), self.others.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c930b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class Trainer():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_paths,\n",
    "                 save_path,\n",
    "                 split = [0,1],\n",
    "                 model_path = 'gpt2',\n",
    "                 stratify = None,\n",
    "                 sample_weights = None,\n",
    "                 batch_size = 16,\n",
    "                 learning_rate = 5e-5,\n",
    "                 num_epochs = 3,\n",
    "                 warmup_percent = 1,\n",
    "                ):\n",
    "        \n",
    "        self.dataset = LM_Datasets(data_paths, split)\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        wrs = WeightedRandomSampler(self.dataset.others['cls_labels'], stratify = stratify, weights = sample_weights)\n",
    "        sampler = torch.utils.data.sampler.BatchSampler(wrs, batch_size=batch_size,drop_last=False)\n",
    "        \n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', use_fast = True)\n",
    "        \n",
    "        self.train_loader = DataLoader(dataset = self.dataset, \n",
    "                                       batch_size = None, \n",
    "                                       collate_fn = self.train_collator, \n",
    "                                       sampler = sampler)\n",
    "        \n",
    "        \n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "        self.model.pad_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_warmup_steps = round(num_epochs * len(self.train_loader) * warmup_percent / (batch_size * 100)) * batch_size\n",
    "        self.num_training_steps = num_epochs * len(self.train_loader)\n",
    "        self.lr_scheduler = get_scheduler(\"linear\", \n",
    "                                     optimizer = self.optimizer, \n",
    "                                     num_warmup_steps = self.num_warmup_steps, \n",
    "                                     num_training_steps = self.num_training_steps)\n",
    "    \n",
    "    def train_collator(self, batch):\n",
    "        max_len = max([len(l) for l in batch[0]['gold_ids']])\n",
    "\n",
    "        pads = {'input_ids': self.tokenizer.eos_token_id, 'labels': -100, 'attention_mask': 0}\n",
    "        keys = {'gold_ids': 'input_ids', 'gold_mask': 'attention_mask', 'lm_labels': 'labels'}\n",
    "        out = {}\n",
    "        for key, val in keys.items():\n",
    "            out[val] = torch.tensor([sample + [pads[val]] * (max_len - len(sample)) for sample in batch[0][key]])\n",
    "        return out\n",
    "    \n",
    "    def train(self, save = True, evaluator = None):\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model.to(device)\n",
    "        \n",
    "        progress_bar_train = tqdm(range(self.num_training_steps))\n",
    "        losses = []\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            #print('epoch: {}'.format(epoch))\n",
    "            for i, batch in enumerate(self.train_loader):\n",
    "                #print('batch: {}'.format(i))\n",
    "                enc = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = self.model(**enc)\n",
    "                loss = outputs.loss\n",
    "                losses += [loss.detach().cpu()]\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.lr_scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                progress_bar_train.update(1)\n",
    "                \n",
    "                if i % round(len(self.train_loader)/3) == 1:\n",
    "                    print(f'Average loss: {sum(losses)/len(losses)}')\n",
    "                    losses = []\n",
    "            if evaluator is not None:\n",
    "                self.model.eval()\n",
    "                evaluator.evaluate(self.model, save)\n",
    "        if save:\n",
    "            self.model.save_pretrained(self.save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f7805",
   "metadata": {},
   "source": [
    "trainer = Trainer(data_paths = [#('../../data/tokenized/gpt2/ecqa/train.csv', 'ecqa'),\n",
    "                               ('../../data/tokenized/gpt2/esnli/train.csv', 'esnli'),\n",
    "                               #('../../data/tokenized/gpt2/comve/train.csv', 'comve')\n",
    "                               ], \n",
    "                  save_path = 'eSNLI/GPT2SingleTask', \n",
    "                  #model_path = 'eSNLI/Gold', \n",
    "                  split = [0,0.5],\n",
    "                  num_epochs = 3, \n",
    "                  batch_size = 4,\n",
    "                  stratify = 'decrease')\n",
    "trainer.train(save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "import json\n",
    "\n",
    "class Evaluator():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_paths,\n",
    "                 save_paths,\n",
    "                 split = [0,1],\n",
    "                 stratify = None,\n",
    "                 sample_weights = None,\n",
    "                 batch_size = 16,\n",
    "                 gold_explanations = True\n",
    "                ):\n",
    "        \n",
    "        self.dataset = LM_Datasets(data_paths, split)\n",
    "        self.save_paths = save_paths\n",
    "        \n",
    "        self.wrs = WeightedRandomSampler(self.dataset.others['cls_labels'], stratify = stratify, weights = sample_weights)\n",
    "        sampler = torch.utils.data.sampler.BatchSampler(self.wrs, batch_size=batch_size,drop_last=False)\n",
    "        \n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', use_fast = True)\n",
    "        self.test_loader = DataLoader(dataset = self.dataset, \n",
    "                                       batch_size = None, \n",
    "                                       collate_fn = self.test_collator, \n",
    "                                       sampler = sampler)\n",
    "        \n",
    "        self.bertscore = load_metric('bertscore')\n",
    "        self.gold_explanations = gold_explanations\n",
    "        \n",
    "    def test_collator(self, batch):\n",
    "        max_len = max([len(l) for l in batch[0]['input_ids']])\n",
    "        \n",
    "        pads = {'input_ids': self.tokenizer.eos_token_id, 'attention_mask': 0}\n",
    "        keys = {'input_ids': 'input_ids', 'attention_mask': 'attention_mask'}\n",
    "        out = {}\n",
    "        for key, val in keys.items():\n",
    "            out[val] = torch.tensor([[pads[val]] * (max_len - len(sample)) + sample for sample in batch[0][key]])\n",
    "            \n",
    "        return out, batch[1]\n",
    "    \n",
    "    def evaluate(self, model, save = False):\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "        model.config.max_length = 150\n",
    "\n",
    "        progress_bar = tqdm(range(len(self.test_loader)))\n",
    "        \n",
    "        self.df = pd.DataFrame()\n",
    "        model.eval()\n",
    "        for i, batch in enumerate(self.test_loader):\n",
    "            enc = {k: v.to(device) for k, v in batch[0].items()}\n",
    "            with torch.no_grad():\n",
    "                generation = model.generate(\n",
    "                    input_ids = enc['input_ids'],\n",
    "                    attention_mask = enc['attention_mask'],\n",
    "                    do_sample = True,\n",
    "                    max_length = 100,\n",
    "                    temperature = 0.7,\n",
    "                    top_k = 50,\n",
    "                    top_p = 0.7\n",
    "                )\n",
    "                batch_input = self.tokenizer.batch_decode(enc['input_ids'].detach(), skip_special_tokens=True)\n",
    "                preds = []\n",
    "                for i, e in zip(batch_input, self.tokenizer.batch_decode(generation.detach(), skip_special_tokens=True)):\n",
    "                    preds += [e[len(i):].strip()]\n",
    "                \n",
    "                if self.gold_explanations: \n",
    "                    try:\n",
    "                        golds = batch[1]['explanations'].tolist()\n",
    "                        self.bertscore.add_batch(predictions=preds, references=golds)\n",
    "                    except:\n",
    "                        continue\n",
    "                batch[1]['generated'] = preds\n",
    "                self.df = self.df.append(batch[1])\n",
    "\n",
    "            progress_bar.update(1)\n",
    "        if save:\n",
    "            for (save_path, data_id) in self.save_paths:\n",
    "                with open(save_path, 'w') as f:\n",
    "                    json.dump(self.df[self.df['data_id'] == data_id].to_dict(), f)\n",
    "                              \n",
    "            if self.gold_explanations:\n",
    "                res = self.bertscore.compute(lang = 'en')\n",
    "                print(f'{round(sum(res[\"precision\"])/len(res[\"precision\"]), 2)}\\t\\t|\\t'+\n",
    "                      f'{round(sum(res[\"recall\"])/len(res[\"recall\"]), 2)}\\t\\t|\\t'+\n",
    "                      f'{round(sum(res[\"f1\"])/len(res[\"f1\"]), 2)}')\n",
    "\n",
    "                with open('../../data/generated/bertscores.json'.format(save_path), 'w') as f:\n",
    "                    json.dump({'Precision': round(sum(res[\"precision\"])/len(res[\"precision\"]), 2),\n",
    "                               'Recall': round(sum(res[\"recall\"])/len(res[\"recall\"]), 2),\n",
    "                               'F1-score': round(sum(res[\"f1\"])/len(res[\"f1\"]), 2)}, \n",
    "                              f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ca351",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(data_paths = [#('../../data/tokenized/gpt2/ecqa/train.csv', 'ecqa'),\n",
    "                                   ('../../data/tokenized/gpt2/esnli/test.csv', 'esnli'),\n",
    "                                   #('../../data/tokenized/gpt2/comve/train.csv', 'comve')\n",
    "                                   ],\n",
    "                      save_paths =  [#('../../data/generated/singletask/shared/ecqa_train.json', 'ecqa'),\n",
    "                                   ('../../data/generated/multitask/separate/esnli_test.json', 'esnli'),\n",
    "                                   #('../../data/generated/singletask/shared/comve_train.json', 'comve')\n",
    "                                    ],\n",
    "                      #split = [0,0.5], \n",
    "                      batch_size = 4,\n",
    "                     gold_explanations = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4185cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('eSNLI/GPT2SingleTask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad4564",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(model, save = True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b5471fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062670b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
