{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b325bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Sampler(object):\n",
    "    r\"\"\"Base class for all Samplers.\n",
    "\n",
    "    Every Sampler subclass has to provide an __iter__ method, providing a way\n",
    "    to iterate over indices of dataset elements, and a __len__ method that\n",
    "    returns the length of the returned iterators.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source):\n",
    "        pass\n",
    "\n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class WeightedRandomSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, labels, stratify = None, weights = None):\n",
    "        self.labels = labels\n",
    "        self.label_counts = labels.value_counts().to_dict()\n",
    "        self.num_labels = len(self.label_counts.keys())\n",
    "        \n",
    "        if weights is None:\n",
    "            self.weights = {key: 1 for key, val in self.label_counts.items()}\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        if stratify == 'increase':\n",
    "            self.samples_per_label = {key: round(max(self.label_counts.values()) * weight) for key, weight in self.weights.items()}\n",
    "        elif stratify == 'decrease':\n",
    "            self.samples_per_label = {key: round(min(self.label_counts.values()) * weight) for key, weight in self.weights.items()}\n",
    "        else:\n",
    "            self.samples_per_label = {key: round(self.label_counts[key] * self.weights[key]) for key in self.weights} \n",
    "        \n",
    "        self.num_samples = sum(self.samples_per_label.values())\n",
    "        \n",
    "    def __iter__(self):\n",
    "        indices = []\n",
    "        for lbl, amount in self.samples_per_label.items():\n",
    "            label_data = self.labels[self.labels == lbl]\n",
    "            label_counts = len(label_data)\n",
    "            for i in range(amount // label_counts):\n",
    "                indices += label_data.index[torch.randperm(label_counts)].tolist()\n",
    "            indices += label_data.index[torch.randperm(amount % label_counts)].tolist()\n",
    "        return iter(np.array(indices)[torch.randperm(self.num_samples)].tolist())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e799e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class CLS_Datasets(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_paths, split = [0,1]):\n",
    "        \n",
    "        self.encodings = pd.DataFrame(columns = ['attention_mask', 'input_ids', 'token_type_ids', 'encoded_labels'])\n",
    "        self.others = pd.DataFrame(columns = ['groups', 'data_id' 'labels'])\n",
    "        for (data_path, data_id) in data_paths:\n",
    "            dataset = pd.read_csv(f'{data_path}', header = 0, index_col = 0).reset_index()\n",
    "            dataset = dataset[round(len(dataset)*split[0]): round(len(dataset)*split[1])]\n",
    "            dataset['data_id'] = data_id\n",
    "            tqdm.pandas()\n",
    "            dataset[['attention_mask', 'input_ids', 'token_type_ids']] = \\\n",
    "            dataset[['attention_mask', 'input_ids', 'token_type_ids']].progress_applymap(literal_eval)\n",
    "            num_labels = len(pd.unique(self.encodings['encoded_labels']))\n",
    "            lbls = np.sort(pd.unique(dataset['labels']))\n",
    "            dataset['encoded_labels'] = dataset['labels'].apply(lambda lbl: np.where(lbls == lbl)[0][0] + num_labels + addit)\n",
    "            self.encodings = self.encodings.append(\\\n",
    "            dataset[['attention_mask', 'input_ids', 'token_type_ids', 'encoded_labels']])\n",
    "            \n",
    "            self.others = self.others.append(dataset.drop(['attention_mask', 'input_ids', 'token_type_ids', 'encoded_labels'], axis=1))\n",
    "        self.encodings = self.encodings.rename(columns = {'encoded_labels': 'labels'})\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encodings.loc[idx].to_dict('list'), self.others.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c930b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class Trainer():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_paths,\n",
    "                 save_path,\n",
    "                 split = [0,1],\n",
    "                 model_path = None,\n",
    "                 stratify = None,\n",
    "                 sample_weights = None,\n",
    "                 batch_size = 16,\n",
    "                 learning_rate = 5e-5,\n",
    "                 num_epochs = 3,\n",
    "                 warmup_percent = 1,\n",
    "                ):\n",
    "        \n",
    "        self.dataset = CLS_Datasets(data_paths, split)\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        data_classes = {}\n",
    "        encs = self.dataset.encodings\n",
    "        othr = self.dataset.others\n",
    "        for data_id in pd.unique(othr['data_id']):\n",
    "            data_classes[data_id] = [label for label in \\\n",
    "                pd.unique(encs['labels'][othr['data_id'] == data_id])]\n",
    "        self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 7)\n",
    "        self.wrs = WeightedRandomSampler(encs['labels'], stratify = stratify, weights = sample_weights)\n",
    "        sampler = torch.utils.data.sampler.BatchSampler(self.wrs, batch_size=batch_size,drop_last=False)\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast = True)\n",
    "        dataCollator = DataCollatorWithPadding(tokenizer = self.tokenizer)\n",
    "        self.train_loader = DataLoader(dataset = self.dataset, \n",
    "                                       batch_size = None, \n",
    "                                       collate_fn = lambda x: dataCollator(x[0]), \n",
    "                                       sampler = sampler)\n",
    "        \n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_warmup_steps = round(num_epochs * len(self.train_loader) * warmup_percent / (batch_size * 100)) * batch_size\n",
    "        self.num_training_steps = num_epochs * len(self.train_loader)\n",
    "        self.lr_scheduler = get_scheduler(\"linear\", \n",
    "                                     optimizer = self.optimizer, \n",
    "                                     num_warmup_steps = self.num_warmup_steps, \n",
    "                                     num_training_steps = self.num_training_steps)\n",
    "    \n",
    "    def train(self, save = True, evaluator = None):\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model.to(device)\n",
    "        progress_bar_train = tqdm(range(self.num_training_steps))\n",
    "        losses = []\n",
    "        current_loss = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            for i, batch in enumerate(self.train_loader):\n",
    "                #if i > 10: break\n",
    "                enc = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = self.model(**enc)\n",
    "                loss = outputs.loss\n",
    "                current_loss = loss.detach().cpu()\n",
    "                losses += [loss.detach().cpu()]\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.lr_scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                progress_bar_train.update(1)\n",
    "                \n",
    "                if i % round(len(self.train_loader)/3) == 1:\n",
    "                    print(f'Average loss: {sum(losses)/len(losses)}')\n",
    "                    losses = []\n",
    "            if evaluator is not None:\n",
    "                self.model.eval()\n",
    "                evaluator.evaluate(self.model)\n",
    "        print(f'Current loss: {current_loss}')\n",
    "        if save:\n",
    "            self.model.save_pretrained(self.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed576588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "class Evaluator():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_paths,\n",
    "                 split = [0,1],\n",
    "                 stratify = None,\n",
    "                 sample_weights = None,\n",
    "                 batch_size = 16\n",
    "                ):\n",
    "        \n",
    "        self.dataset = CLS_Datasets(data_paths, split)\n",
    "        wrs = WeightedRandomSampler(self.dataset.encodings['labels'], stratify = stratify, weights = sample_weights)\n",
    "        sampler = torch.utils.data.sampler.BatchSampler(wrs, batch_size=batch_size,drop_last=False)\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast = True)\n",
    "        dataCollator = DataCollatorWithPadding(tokenizer = self.tokenizer)\n",
    "        self.eval_loader = DataLoader(dataset = self.dataset, \n",
    "                                       batch_size = None, \n",
    "                                       collate_fn = lambda x: (dataCollator(x[0]), x[1]), \n",
    "                                       sampler = sampler)\n",
    "        \n",
    "    \n",
    "    def evaluate(self, model):\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        data_ids = []\n",
    "        groups = []\n",
    "        lgits = []\n",
    "        preds = []\n",
    "        refs = []\n",
    "        opts = []\n",
    "\n",
    "        model.eval()\n",
    "        self.progress_bar_eval = tqdm(range(len(self.eval_loader)))\n",
    "        self.progress_bar_eval.reset()\n",
    "        for i, (enc, others) in enumerate(self.eval_loader):\n",
    "            #if i > 10: break\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**enc)\n",
    "            logits = outputs.logits\n",
    "                \n",
    "            vals, predictions = torch.max(logits, dim=-1)\n",
    "            data_ids += others['data_id'].tolist()\n",
    "            groups += others['groups'].tolist()\n",
    "            lgits += vals.detach().cpu()\n",
    "            preds += predictions.detach().cpu()\n",
    "            refs += enc['labels'].detach().cpu()\n",
    "\n",
    "            self.progress_bar_eval.update(1)\n",
    "        \n",
    "        df = pd.DataFrame({'data_ids': data_ids, 'groups': groups, 'logits': lgits, 'preds': preds, 'refs': refs})\n",
    "\n",
    "        prec, rec, f1, dist = metrics.precision_recall_fscore_support(refs, preds, average=None)\n",
    "        print(\"-\"*20 + \"EVALUATION\" + \"-\"*20)\n",
    "        print('Precision: \\t\\t{}'.format(prec))\n",
    "        print('Recall: \\t\\t{}'.format(rec))\n",
    "        print('F1: \\t\\t\\t{}'.format(f1))\n",
    "        print('Distribution: \\t\\t{}'.format(dist))\n",
    "        \n",
    "        for data_id in pd.unique(self.dataset.others['data_id']):\n",
    "            d = df[df['data_ids'] == data_id]\n",
    "            #int(\"Original task ({}): \\t\\t{}\".format(data_id, d[(d.groupby('groups'))['logits_bin'].transform(max) == d['logits_bin']]['refs'].mean()))\n",
    "        \n",
    "            tmp = d[(d.groupby('groups'))['logits'].transform(max) == d['logits']]\n",
    "            print(\"Original task ({}) v2: \\t{}\".format(data_id, len(tmp[tmp['preds'] == tmp['refs']].groupby('groups').first())/len(d.groupby('groups').first())))\n",
    "\n",
    "        print(\"-\"*20 + \"----------\" + \"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd4a439",
   "metadata": {},
   "source": [
    "evaluator = Evaluator(data_paths = [#('../../data/tokenized/bert/ecqa/test.csv', 'ecqa'),\n",
    "                                    #('../../data/tokenized/bert/esnli/test.csv', 'esnli'),\n",
    "                                    #('../../data/generated/multitask/separate/comve_test_bert.csv', 'ecqa')\n",
    "                                   ],\n",
    "                      #split = [0,0.01], \n",
    "                      batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51403c16",
   "metadata": {},
   "source": [
    "trainer = Trainer(data_paths = [('../../data/tokenized/bert/ecqa/train.csv', 'ecqa'),\n",
    "                               ('../../data/tokenized/bert/ecqa/train.csv', 'esnli'),\n",
    "                               ('../../data/tokenized/bert/comve/train.csv', 'comve')\n",
    "                                ], \n",
    "                  save_path = 'Shared/Gold', \n",
    "                  model_path = 'eSNLI/Gold', \n",
    "                  split = [0,0.5],\n",
    "                  num_epochs = 3, \n",
    "                  batch_size = 4,\n",
    "                  stratify = 'decrease')\n",
    "trainer.train(save = True)#, evaluator = evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa03d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('Shared/Gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69622a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "addit = 2\n",
    "for sepsh in ['separate', 'shared']:\n",
    "    for stmt in ['singletask', 'multitask']:\n",
    "        try:\n",
    "            evaluator = Evaluator(data_paths = [('../../data/generated/{}/{}/esnli_test_bert.csv'.format(stmt, sepsh), 'esnli')], \n",
    "                          #split = [0.0,0.001], \n",
    "                          batch_size = 32)\n",
    "            evaluator.evaluate(model)\n",
    "            print('SUCCESS: ../../data/generated/{}/{}/esnli_test_bert.csv'.format(stmt, sepsh))\n",
    "        except Exception as e:\n",
    "            print('FAILED: ../../data/generated/{}/{}/esnli_test_bert.csv'.format(stmt, sepsh))\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84ca8d",
   "metadata": {},
   "source": [
    "\n",
    "evaluator.evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3820b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "3814 / (3814 + 7464)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
