{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ed066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Sampler(object):\n",
    "    r\"\"\"Base class for all Samplers.\n",
    "\n",
    "    Every Sampler subclass has to provide an __iter__ method, providing a way\n",
    "    to iterate over indices of dataset elements, and a __len__ method that\n",
    "    returns the length of the returned iterators.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source):\n",
    "        pass\n",
    "\n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class WeightedRandomSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, labels, stratify = None, weights = None):\n",
    "        self.labels = labels\n",
    "        self.label_counts = labels.value_counts().to_dict()\n",
    "        self.num_labels = len(self.label_counts.keys())\n",
    "        \n",
    "        if weights is None:\n",
    "            self.weights = {key: 1 for key, val in self.label_counts.items()}\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        if stratify == 'increase':\n",
    "            self.samples_per_label = {key: round(max(self.label_counts.values()) * weight) for key, weight in self.weights.items()}\n",
    "        elif stratify == 'decrease':\n",
    "            self.samples_per_label = {key: round(min(self.label_counts.values()) * weight) for key, weight in self.weights.items()}\n",
    "        else:\n",
    "            self.samples_per_label = {key: round(self.label_counts[key] * self.weights[key]) for key in self.weights} \n",
    "        \n",
    "        self.num_samples = sum(self.samples_per_label.values())\n",
    "        \n",
    "    def __iter__(self):\n",
    "        indices = []\n",
    "        for lbl, amount in self.samples_per_label.items():\n",
    "            label_data = self.labels[self.labels == lbl]\n",
    "            label_counts = len(label_data)\n",
    "            for i in range(amount // label_counts):\n",
    "                indices += label_data.index[torch.randperm(label_counts)].tolist()\n",
    "            indices += label_data.index[torch.randperm(amount % label_counts)].tolist()\n",
    "        return iter(np.array(indices)[torch.randperm(self.num_samples)].tolist())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DatasetForMultiTaskLearning(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_paths, split = [0,1]):\n",
    "        \n",
    "        self.encodings = pd.DataFrame(columns = ['cls_labels'])\n",
    "        self.others = pd.DataFrame()\n",
    "        for (data_path, data_id) in data_paths:\n",
    "            dataset = pd.read_csv(f'{data_path}', header = 0, index_col = 0)\n",
    "            dataset = dataset[round(len(dataset)*split[0]): round(len(dataset)*split[1])]\n",
    "            #dataset = dataset.dropna().groupby([sen1, sen2]).first().reset_index()\n",
    "            dataset['data_id'] = data_id\n",
    "            lit_evals = ['input_ids', 'attention_mask']\n",
    "            if 'gold_ids' in dataset:\n",
    "                lit_evals += ['gold_ids', 'gold_mask']\n",
    "    \n",
    "            tqdm.pandas()        \n",
    "            dataset[lit_evals] = dataset[lit_evals].progress_applymap(literal_eval)\n",
    "            \n",
    "            \n",
    "            num_labels = len(pd.unique(self.encodings['cls_labels']))\n",
    "            \n",
    "            lbls = np.sort(pd.unique(dataset['labels']))\n",
    "            \n",
    "            dataset['cls_labels'] = dataset['labels'].apply(lambda lbl: np.where(lbls == lbl)[0][0] + num_labels)\n",
    "            tmp = dataset[dataset['cls_labels'] == 1].groupby(['groups']).first().reset_index()\n",
    "            dataset = dataset[dataset['cls_labels'] == 0].append(tmp).reset_index()\n",
    "            #dataset['cls_labels_ids'] = [lbls + num_labels] * len(dataset)\n",
    "            \n",
    "            dataset['cls_labels'] += addit\n",
    "        \n",
    "            self.encodings = self.encodings.append(dataset[lit_evals + ['cls_labels']])\n",
    "            self.others = self.others.append(dataset.drop(lit_evals + ['cls_labels'], axis=1))\n",
    "        if 'gold_ids' in self.encodings:\n",
    "            lm_labels = lambda x: [-100]*len(x['input_ids']) + x['gold_ids'][(len(x['input_ids'])):]\n",
    "            self.encodings['lm_labels'] = self.encodings.apply(lm_labels, axis=1)\n",
    "        self.encodings = self.encodings.reset_index()\n",
    "        self.others = self.others.reset_index()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encodings.loc[idx].to_dict('list'), self.others.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "from transformers import GPT2PreTrainedModel\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "from transformers import GPT2Config\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import torch\n",
    "\n",
    "class GPT2ForMultiTaskConfig(GPT2Config):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_labels = 2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "@dataclass\n",
    "class GPT2ForMultiTaskOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    cls_loss: Optional[torch.FloatTensor] = None\n",
    "    cls_logits: torch.FloatTensor = None\n",
    "\n",
    "class GPT2ForMultiTaskLearning(GPT2PreTrainedModel):\n",
    "    \n",
    "    config_class = GPT2ForMultiTaskConfig\n",
    "    _keys_to_ignore_on_load_missing = [r\"attn.masked_bias\", r\"attn.bias\", r\"lm_head.weight\"]\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.n_labels\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        #self.lm_model = GPT2LMHeadModel.from_pretrained('gpt2', config=config)\n",
    "        self.cls_head = nn.Linear(config.n_embd, config.n_labels, bias=False)\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        #self.post_init()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        lm_labels=None,\n",
    "        cls_labels=None,\n",
    "    ):\n",
    "        \n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        #print(hidden_states.size())\n",
    "        #print(hidden_states)\n",
    "        \n",
    "        lm_logits = self.lm_head(hidden_states)        \n",
    "        cls_logits = self.cls_head(hidden_states)\n",
    "        #rint(cls_logits)\n",
    "        batch_size, sequence_length = input_ids.shape[:2]\n",
    "        sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n",
    "        cls_logits = cls_logits[torch.arange(batch_size, device=self.device), sequence_lengths]\n",
    "        \n",
    "        #rint(cls_logits)\n",
    "        \n",
    "        lm_loss = None\n",
    "        if lm_labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = lm_labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            #lm_loss /= torch.ne(shift_labels, -100).sum(-1).float().mean()\n",
    "\n",
    "        cls_loss = None\n",
    "        if cls_labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            cls_loss = loss_fct(cls_logits.view(-1, self.num_labels), cls_labels.view(-1))\n",
    "\n",
    "        #if not return_dict:\n",
    "        #    output = (lm_logits,) + transformer_outputs[1:]\n",
    "        #    return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return GPT2ForMultiTaskOutput(\n",
    "            loss=lm_loss,\n",
    "            cls_loss=cls_loss,\n",
    "            logits=lm_logits,\n",
    "            cls_logits=cls_logits,\n",
    "            #past_key_values=transformer_outputs.past_key_values,\n",
    "            #hidden_states=transformer_outputs.hidden_states,\n",
    "            #attentions=transformer_outputs.attentions,\n",
    "            #cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class Trainer():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_paths,\n",
    "                 save_path,\n",
    "                 eval_paths = None,\n",
    "                 split = [0,1],\n",
    "                 model_path = 'gpt2',\n",
    "                 stratify = None,\n",
    "                 sample_weights = None,\n",
    "                 batch_size = 16,\n",
    "                 learning_rate = 5e-5,\n",
    "                 num_epochs = 3,\n",
    "                 warmup_percent = 1,\n",
    "                ):\n",
    "        \n",
    "        self.dataset = DatasetForMultiTaskLearning(data_paths, split)\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        wrs = WeightedRandomSampler(self.dataset.encodings['cls_labels'], stratify = stratify, weights = sample_weights)\n",
    "        sampler = torch.utils.data.sampler.BatchSampler(wrs, batch_size=batch_size,drop_last=False)\n",
    "        \n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', use_fast = True)\n",
    "        \n",
    "        self.train_loader = DataLoader(dataset = self.dataset, \n",
    "                                       batch_size = None, \n",
    "                                       collate_fn = self.train_collator, \n",
    "                                       sampler = sampler)\n",
    "        self.eval_loader = None\n",
    "        self.eval_dataset = None\n",
    "        if eval_paths is not None:\n",
    "            self.eval_dataset = DatasetForMultiTaskLearning(eval_paths, split)\n",
    "            wrs = WeightedRandomSampler(self.eval_dataset.encodings['cls_labels'], stratify = stratify, weights = sample_weights)\n",
    "            sampler = torch.utils.data.sampler.BatchSampler(wrs, batch_size=batch_size,drop_last=False)\n",
    "            self.eval_loader = DataLoader(dataset = self.eval_dataset, \n",
    "                                       batch_size = None, \n",
    "                                       collate_fn = self.train_collator, \n",
    "                                       sampler = sampler)\n",
    "        num_labels = len(pd.unique(self.dataset.encodings['cls_labels']))\n",
    "        config = GPT2ForMultiTaskConfig(n_labels = num_labels, pad_token_id = self.tokenizer.eos_token_id)\n",
    "        self.model = GPT2ForMultiTaskLearning.from_pretrained(model_path, config = config)\n",
    "        \n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_warmup_steps = 0#round(num_epochs * len(self.train_loader) * warmup_percent / (batch_size * 100)) * batch_size\n",
    "        self.num_training_steps = num_epochs * len(self.train_loader)\n",
    "        self.lr_scheduler = get_scheduler(\"linear\", \n",
    "                                     optimizer = self.optimizer, \n",
    "                                     num_warmup_steps = self.num_warmup_steps, \n",
    "                                     num_training_steps = self.num_training_steps)\n",
    "    \n",
    "    def train_collator(self, batch):\n",
    "        max_len = max([len(l) for l in batch[0]['gold_ids']])\n",
    "        \n",
    "        pads = {'input_ids': self.tokenizer.eos_token_id, 'lm_labels': -100, 'attention_mask': 0}\n",
    "        keys = {'gold_ids': 'input_ids', 'gold_mask': 'attention_mask', 'lm_labels': 'lm_labels'}\n",
    "        out = {}\n",
    "        for key, val in keys.items():\n",
    "            out[val] = torch.tensor([sample + [pads[val]] * (max_len - len(sample)) for sample in batch[0][key]])\n",
    "        out['cls_labels'] = torch.tensor(batch[0]['cls_labels'])\n",
    "        return out\n",
    "    \n",
    "    def train(self, save = True):\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model.to(device)\n",
    "        \n",
    "        progress_bar_train = tqdm(range(self.num_training_steps))\n",
    "        self.losses = []\n",
    "        for epoch in range(self.num_epochs):\n",
    "            if self.eval_loader is not None:\n",
    "                self.evaluate()\n",
    "            self.model.train()\n",
    "            for i, batch in enumerate(self.train_loader):\n",
    "                enc = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                outputs = self.model(**enc)\n",
    "                loss = outputs.loss + outputs.cls_loss\n",
    "                #loss = outputs.cls_loss\n",
    "                self.losses += [loss.detach().cpu()]\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.lr_scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                progress_bar_train.update(1)\n",
    "                \n",
    "                if i % round(len(self.train_loader)/3) == 1:\n",
    "                    print(f'Average training loss: {sum(self.losses)/len(self.losses)}')\n",
    "                    \n",
    "            if save:\n",
    "                self.model.save_pretrained(f'{self.save_path}_{epoch}')\n",
    "                \n",
    "        if self.eval_loader is not None:\n",
    "            self.evaluate()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model.eval()\n",
    "        lm_losses = []\n",
    "        cls_losses = []\n",
    "        cls_preds = []\n",
    "        cls_true = []\n",
    "        progress_bar_eval = tqdm(range(len(self.eval_loader)))\n",
    "        for i, batch in enumerate(self.eval_loader):\n",
    "            enc = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**enc)\n",
    "            lm_losses += [outputs.loss.detach().cpu()]\n",
    "            cls_losses += [outputs.cls_loss.detach().cpu()]\n",
    "            vals, predictions = torch.max(outputs.cls_logits, dim=-1)\n",
    "            cls_preds += predictions.detach().cpu().tolist()#outputs.cls_logits.detach().cpu().tolist()\n",
    "            cls_true += enc['cls_labels'].cpu().tolist()\n",
    "            progress_bar_eval.update(1)\n",
    "\n",
    "        print(f'Average language modeling loss: {sum(lm_losses)/len(lm_losses)}')\n",
    "        print(f'Average classification loss: {sum(cls_losses)/len(cls_losses)}')\n",
    "        print(f'Average classification accuracy: {sum([x == y for x, y in zip(cls_preds, cls_true)])/len(cls_true)}')\n",
    "        losses = []\n",
    "        alpha = (sum(lm_losses)/len(lm_losses)) / (sum(cls_losses)/len(cls_losses))\n",
    "        print(f'Alpha = {alpha}')\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(data_paths = [('../../data/generated/multitask/separate/ecqa_train_gpt.csv', 'ecqa'),\n",
    "#trainer = Trainer(data_paths = [('../../data/tokenized/gpt2/ecqa/train.csv', 'ecqa'),\n",
    "                               #('../../data/tokenized/gpt2/esnli/train.csv', 'esnli'),\n",
    "                               #('../../data/tokenized/gpt2/comve/train.csv', 'comve')\n",
    "                               ],\n",
    "                  #eval_paths = [('../../data/tokenized/gpt2/ecqa/train.csv', 'ecqa'),\n",
    "                  #eval_paths = [#('../../data/tokenized/gpt2/ecqa/dev.csv', 'ecqa'),\n",
    "                               #('../../data/tokenized/gpt2/esnli/dev.csv', 'esnli'),\n",
    "                               #('../../data/tokenized/gpt2/comve/dev.csv', 'comve')\n",
    "                  #            ], \n",
    "                  save_path = 'eSNLI/Gen2_notft', \n",
    "                  #model_path = 'eSNLI/Gold', \n",
    "                  #split = [0,0.5],\n",
    "                  num_epochs = 3, \n",
    "                  batch_size = 4,\n",
    "                  stratify = 'decrease')\n",
    "trainer.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(trainer.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb71193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "import json\n",
    "\n",
    "class Generator():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_paths,\n",
    "                 save_paths,\n",
    "                 split = [0,1],\n",
    "                 stratify = None,\n",
    "                 sample_weights = None,\n",
    "                 batch_size = 16,\n",
    "                 gold_explanations = True\n",
    "                ):\n",
    "        \n",
    "        self.dataset = DatasetForMultiTaskLearning(data_paths, split)\n",
    "        self.save_paths = save_paths\n",
    "        \n",
    "        self.wrs = WeightedRandomSampler(self.dataset.encodings['cls_labels'], stratify = stratify, weights = sample_weights)\n",
    "        sampler = torch.utils.data.sampler.BatchSampler(self.wrs, batch_size=batch_size,drop_last=False)\n",
    "        \n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', use_fast = True)\n",
    "        self.test_loader = DataLoader(dataset = self.dataset, \n",
    "                                       batch_size = None, \n",
    "                                       collate_fn = self.test_collator, \n",
    "                                       sampler = sampler)\n",
    "        \n",
    "        self.bertscore = load_metric('bertscore')\n",
    "        self.gold_explanations = gold_explanations\n",
    "        \n",
    "    def test_collator(self, batch):\n",
    "        max_len = max([len(l) for l in batch[0]['input_ids']])\n",
    "        \n",
    "        pads = {'input_ids': self.tokenizer.eos_token_id, 'attention_mask': 0}\n",
    "        keys = {'input_ids': 'input_ids', 'attention_mask': 'attention_mask'}\n",
    "        out = {}\n",
    "        for key, val in keys.items():\n",
    "            out[val] = torch.tensor([[pads[val]] * (max_len - len(sample)) + sample for sample in batch[0][key]])\n",
    "            \n",
    "        return out, batch[1]\n",
    "    \n",
    "    def generate(self, model, save = False):\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        progress_bar = tqdm(range(len(self.test_loader)))\n",
    "        \n",
    "        self.df = pd.DataFrame()\n",
    "        model.eval()\n",
    "        for i, batch in enumerate(self.test_loader):\n",
    "            enc = {k: v.to(device) for k, v in batch[0].items()}\n",
    "            with torch.no_grad():\n",
    "                generation = model.generate(\n",
    "                    input_ids = enc['input_ids'],\n",
    "                    attention_mask = enc['attention_mask'],\n",
    "                    do_sample = True,\n",
    "                    max_length = 100,\n",
    "                    temperature = 0.7,\n",
    "                    top_k = 50,\n",
    "                    top_p = 0.7\n",
    "                )\n",
    "                batch_input = self.tokenizer.batch_decode(enc['input_ids'].detach(), skip_special_tokens=True)\n",
    "                preds = []\n",
    "                for i, e in zip(batch_input, self.tokenizer.batch_decode(generation.detach(), skip_special_tokens=True)):\n",
    "                    preds += [e[len(i):].strip()]\n",
    "                \n",
    "                if self.gold_explanations: \n",
    "                    try:\n",
    "                        golds = batch[1]['explanations'].tolist()\n",
    "                        self.bertscore.add_batch(predictions=preds, references=golds)\n",
    "                    except:\n",
    "                        continue\n",
    "                batch[1]['generated'] = preds\n",
    "                self.df = self.df.append(batch[1])\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            if save:\n",
    "                for (save_path, data_id) in self.save_paths:\n",
    "                    with open(save_path, 'w') as f:\n",
    "                        json.dump(self.df[self.df['data_id'] == data_id].to_dict('list'), f)\n",
    "                              \n",
    "            if self.gold_explanations:\n",
    "                res = self.bertscore.compute(lang = 'en')\n",
    "                print(f'{round(sum(res[\"precision\"])/len(res[\"precision\"]), 2)}\\t\\t|\\t'+\n",
    "                      f'{round(sum(res[\"recall\"])/len(res[\"recall\"]), 2)}\\t\\t|\\t'+\n",
    "                      f'{round(sum(res[\"f1\"])/len(res[\"f1\"]), 2)}')\n",
    "\n",
    "                with open('../../data/generated/bertscores.json'.format(save_path), 'w') as f:\n",
    "                    json.dump({'Precision': round(sum(res[\"precision\"])/len(res[\"precision\"]), 2),\n",
    "                               'Recall': round(sum(res[\"recall\"])/len(res[\"recall\"]), 2),\n",
    "                               'F1-score': round(sum(res[\"f1\"])/len(res[\"f1\"]), 2)}, \n",
    "                              f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f65e8",
   "metadata": {},
   "source": [
    "generator = Generator(data_paths = [#('../../data/tokenized/gpt2/ecqa/train.csv', 'ecqa'),\n",
    "                                   ('../../data/tokenized/gpt2/esnli/train.csv', 'esnli'),\n",
    "                                   #('../../data/tokenized/gpt2/comve/test.csv', 'comve')\n",
    "                                    ],\n",
    "                      save_paths =  [#('../../data/generated/multitask/shared/ecqa_train.json', 'ecqa'),\n",
    "                                   ('../../data/generated/multitask/separate/esnli_train.json', 'esnli'),\n",
    "                                   #('../../data/generated/multitask/separate/comve_test.json', 'comve')\n",
    "                                    ],\n",
    "                      split = [0,0.5], \n",
    "                      batch_size = 8,\n",
    "                     gold_explanations = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df89628",
   "metadata": {},
   "source": [
    "model = GPT2ForMultiTaskLearning.from_pretrained('eSNLI/GPT2MultiTask_2')#GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7317dfcc",
   "metadata": {},
   "source": [
    "generator.tokenizer(f\"Statement: Where is the best place to keep ice crean?\\nStatement: party\\nExplanation:\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be28da7",
   "metadata": {},
   "source": [
    "model.to('cpu')\n",
    "out = model.generate(**generator.tokenizer(f\"Statement: Where is the best place to keep ice crean?\\nStatement: party\\nExplanation:\", return_tensors='pt'), max_length = 200)\n",
    "generator.tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a3d77",
   "metadata": {},
   "source": [
    "generator.generate(model, save = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284973a5",
   "metadata": {},
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e91b90",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('../../data/generated/multitask/separate/ecqa_train.json', 'r') as f:\n",
    "    ecqa_generated = pd.DataFrame(json.load(f))\n",
    "for sample in ecqa_generated.groupby('labels').sample(n=2).iterrows():\n",
    "    sample = sample[1]\n",
    "    print('Questions: {}\\nOptions: {}\\nLabel: {}\\nExplanation: {}\\nGenerated: {}\\n\\n'.format(sample['questions'],sample['options'],sample['labels'],sample['explanations'],sample['generated']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b81992c",
   "metadata": {},
   "source": [
    "for sample in esnli_generated.groupby('labels').sample(n=2).iterrows():\n",
    "    sample = sample[1]\n",
    "    print('Premise: {}\\nHypothesis: {}\\nLabel: {}\\nExplanation: {}\\nGenerated: {}\\n\\n'.format(sample['premise'],sample['hypothesis'],sample['labels'],sample['explanations'],sample['generated']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898f7d1",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('../../data/generated/multitask/separate/comve_test.json', 'r') as f:\n",
    "    comve_generated = pd.DataFrame(json.load(f))\n",
    "for sample in comve_generated.groupby('labels').sample(n=2).iterrows():\n",
    "    sample = sample[1]\n",
    "    print('Correct: {}\\nIncorrect: {}\\nLabel: {}\\nExplanation: {}\\nGenerated: {}\\n\\n'.format(sample['correct'],sample['incorrect'],sample['labels'],sample['explanations'],sample['generated']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "class Evaluator():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_paths,\n",
    "                 split = [0,1],\n",
    "                 stratify = None,\n",
    "                 sample_weights = None,\n",
    "                 batch_size = 16,\n",
    "                ):\n",
    "        \n",
    "        self.dataset = DatasetForMultiTaskLearning(data_paths, split)\n",
    "        \n",
    "        self.wrs = WeightedRandomSampler(self.dataset.encodings['cls_labels'], stratify = stratify, weights = sample_weights)\n",
    "        sampler = torch.utils.data.sampler.BatchSampler(self.wrs, batch_size=batch_size,drop_last=False)\n",
    "        \n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', use_fast = True)\n",
    "        self.eval_loader = DataLoader(dataset = self.dataset, \n",
    "                                       batch_size = None, \n",
    "                                       collate_fn = self.eval_collator, \n",
    "                                       sampler = sampler)\n",
    "    \n",
    "    def eval_collator(self, batch):\n",
    "        max_len = max([len(l) for l in batch[0]['attention_mask']])\n",
    "        \n",
    "        pads = {'input_ids': self.tokenizer.eos_token_id, 'attention_mask': 0}\n",
    "        keys = {'input_ids': 'input_ids', 'attention_mask': 'attention_mask'}\n",
    "        out = {}\n",
    "        for key, val in keys.items():\n",
    "            out[val] = torch.tensor([sample + [pads[val]] * (max_len - len(sample)) for sample in batch[0][key]])\n",
    "        out['cls_labels'] = torch.tensor(batch[0]['cls_labels'])\n",
    "        return out, batch[1]\n",
    "    \n",
    "    def evaluate(self, model):\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "        \n",
    "        groups = []\n",
    "        lgits = []\n",
    "        lgits_bin1 = []\n",
    "        lgits_bin6 = []\n",
    "        preds = []\n",
    "        refs = []\n",
    "        opts = []\n",
    "        data_ids = []\n",
    "\n",
    "        model.eval()\n",
    "        self.progress_bar_eval = tqdm(range(len(self.eval_loader)))\n",
    "        self.progress_bar_eval.reset()\n",
    "        for i, (enc, others) in enumerate(self.eval_loader):\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**enc)\n",
    "            logits = outputs.cls_logits\n",
    "            vals, predictions = torch.max(logits, dim=-1)\n",
    "            groups += others['groups'].tolist()\n",
    "            lgits_bin1 += logits[:,1].detach().cpu()\n",
    "            #lgits_bin6 += logits[:,6].detach().cpu()\n",
    "            lgits += vals.detach().cpu()\n",
    "            preds += predictions.detach().cpu()\n",
    "            refs += enc['cls_labels'].tolist()\n",
    "            data_ids += others['data_id'].tolist()\n",
    "\n",
    "            self.progress_bar_eval.update(1)\n",
    "            \n",
    "        df = pd.DataFrame({'data_ids': data_ids, 'groups': groups, 'logits': lgits, 'logits_bin1': lgits_bin1, 'preds': preds, 'refs': refs})\n",
    "        #df = pd.DataFrame({'data_ids': data_ids, 'groups': groups, 'logits': lgits, 'logits_bin1': lgits_bin1,'logits_bin6': lgits_bin6, 'preds': preds, 'refs': refs})\n",
    "        \n",
    "        print(\"-\"*20 + \"----------\" + \"-\"*20)\n",
    "        prec, rec, f1, dist = metrics.precision_recall_fscore_support(refs, preds, average=None)\n",
    "        print(\"-\"*20 + \"EVALUATION\" + \"-\"*20)\n",
    "        print('Precision: \\t\\t{}'.format(prec))\n",
    "        print('Recall: \\t\\t{}'.format(rec))\n",
    "        print('F1: \\t\\t\\t{}'.format(f1))\n",
    "        print('Distribution: \\t\\t{}'.format(dist))\n",
    "        \n",
    "        ecqa_task = df[(df.groupby('groups'))['logits_bin1'].transform(max) == df['logits_bin1']]\n",
    "        ecqa_task = ecqa_task[ecqa_task['data_ids'] == 'ecqa']\n",
    "        print(\"Original task ecqa: \\t\\t{}\".format(ecqa_task['refs'].mean()))\n",
    "        #comve_task = df[(df.groupby('groups'))['logits_bin1'].transform(max) == df['logits_bin1']]\n",
    "        #comve_task = comve_task[comve_task['data_ids'] == 'comve']\n",
    "        #print(\"Original task comve: \\t\\t{}\".format((comve_task['refs'] - 0).mean()))\n",
    "        \n",
    "        tmp = df[(df.groupby('groups'))['logits'].transform(max) == df['logits']]\n",
    "        tmp = tmp[tmp['data_ids'] == 'esnli']\n",
    "        print(\"Original task v2: \\t{}\".format(len(tmp[tmp['preds'] == tmp['refs']].groupby('groups').first())/len(df.groupby('groups').first())))\n",
    "        \n",
    "        print(\"-\"*20 + \"----------\" + \"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0e4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2ForMultiTaskLearning.from_pretrained('ECQA/GPT2MultiTask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = 'questions'\n",
    "sen2 = 'options'\n",
    "addit = 0\n",
    "evaluator = Evaluator(data_paths = [('../../data/generated/multitask/separate/ecqa_test_gpt.csv', 'ecqa'),\n",
    "                                   #('../../data/tokenized/gpt2/esnli/test.csv', 'esnli'),\n",
    "                                   #('../../data/tokenized/gpt2/comve/test.csv', 'comve')\n",
    "                                    ],\n",
    "                      #split = [0,0.5], \n",
    "                      batch_size = 8,\n",
    "                     )\n",
    "evaluator.evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36926aee",
   "metadata": {},
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "addit = 0\n",
    "for sepsh in ['separate', 'shared']:\n",
    "    for stmt in ['singletask', 'multitask']:\n",
    "        try:\n",
    "            evaluator = Evaluator(data_paths = [('../../data/generated/{}/{}/comve_test_gpt.csv'.format(stmt, sepsh), 'comve')], \n",
    "                          #split = [0.0,0.001], \n",
    "                          batch_size = 8)\n",
    "            evaluator.evaluate(model)\n",
    "            print('SUCCESS: ../../data/generated/{}/{}/comve_test_gpt.csv'.format(stmt, sepsh))\n",
    "        except Exception as e:\n",
    "            print('FAILED: ../../data/generated/{}/{}/comve_test_gpt.csv'.format(stmt, sepsh))\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd27f0c",
   "metadata": {},
   "source": [
    "evaluator.dataset.encodings['cls_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf7313",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "evaluator.evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d485cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
