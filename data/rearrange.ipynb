{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03031185",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e4c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# installations\n",
    "! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b0bf3",
   "metadata": {},
   "source": [
    "Import ECQA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ecqa_train = pd.read_csv('raw/ecqa/train.csv', header = 0, index_col = 0)\n",
    "ecqa_dev = pd.read_csv('raw/ecqa/dev.csv', header = 0, index_col = 0)\n",
    "ecqa_test = pd.read_csv('raw/ecqa/test.csv', header = 0, index_col = 0)\n",
    "\n",
    "esnli_train = pd.concat([pd.read_csv('raw/esnli/train1.csv', header = 0, index_col = 0),\\\n",
    "                          pd.read_csv('raw/esnli/train2.csv', header = 0, index_col = 0)])\n",
    "esnli_dev = pd.read_csv('raw/esnli/dev.csv', header = 0, index_col = 0)\n",
    "esnli_test = pd.read_csv('raw/esnli/test.csv', header = 0, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dde32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_ecqa(groups):\n",
    "    idx = groups.index\n",
    "    \n",
    "    neg = str.splitlines(groups['taskA_neg'][idx[0]])\n",
    "    neg_idx = idx[groups['q_ans'] != groups['q_op']]\n",
    "    \n",
    "    pos_idx = idx[groups['q_ans'] == groups['q_op']]\n",
    "    groups['labels'][pos_idx] = 1\n",
    "    \n",
    "    # It is impossible to automate the pairing of option and explanation if they doesn't match\n",
    "    # drop_explanations should be False when evaluating original task. \n",
    "    if len(neg_idx) != len(neg):\n",
    "        groups['drop_exp'] = True\n",
    "        return groups\n",
    "    \n",
    "    for local_idx, global_idx in enumerate(neg_idx):\n",
    "        groups['explanations'][global_idx] = neg[local_idx]\n",
    "    \n",
    "    pos = str.splitlines(groups['taskA_pos'][idx[0]])\n",
    "    pos_grp = groups[groups['q_ans'] == groups['q_op']]\n",
    "    add = len(pos) - len(pos_grp)\n",
    "    \n",
    "    # We duplicate the correct answer with an unique explanation\n",
    "    groups = groups.append([pos_grp]*(add), ignore_index=True) if len(pos_grp) == 1 else \\\n",
    "    groups.append([pos_grp.iloc(0)]*(add), ignore_index=True)\n",
    "    \n",
    "    idx = groups.index\n",
    "    pos_idx = idx[groups['q_ans'] == groups['q_op']]\n",
    "    \n",
    "    for local_idx, global_idx in enumerate(pos_idx):\n",
    "        groups['explanations'][global_idx] = pos[min(local_idx,len(pos)-1)]\n",
    "   \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f148f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def reformat_ecqa(df):\n",
    "    # Copy data\n",
    "    df = df.copy()\n",
    "\n",
    "    # Format the data such every option gets an unique row\n",
    "    id_columns = [\"q_no\",\"q_concept\",\"q_text\",\"q_ans\",'taskA_neg','taskA_pos']\n",
    "    df = pd.wide_to_long(df, \"q_op\", i=id_columns, j=\"option\").reset_index()\n",
    "    df = df.assign(explanations = \"\", drop_exp = False, labels = 0)\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    df = df.groupby('q_no').progress_apply(seperate_ecqa).reset_index(drop=True)\n",
    "    df = df.rename(columns={\"q_no\": \"groups\", \"q_text\": \"questions\", \"q_op\": \"options\", \"option\": \"option_no\"})\n",
    "    df[['questions', 'options','explanations']] = df[['questions', 'options','explanations']].applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    return df[['groups', 'questions', 'options', 'explanations', 'option_no', 'labels', 'drop_exp']]\n",
    "    \n",
    "ecqa_train_rf = reformat_ecqa(ecqa_train)\n",
    "ecqa_dev_rf = reformat_ecqa(ecqa_dev)\n",
    "ecqa_test_rf = reformat_ecqa(ecqa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def reformat_esnli(esnli):\n",
    "    # Copy data\n",
    "    df = esnli.copy()\n",
    "    df['groups'] = df.index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # Format the data such every option gets an unique row\n",
    "    id_columns = ['groups']\n",
    "    df = pd.wide_to_long(df, \"Explanation\", i=id_columns, j=\"explanation_no\", sep=\"_\").reset_index()\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    #df['id'] = df['id'].progress_apply(lambda x: x[:-1])\n",
    "    df = df.rename(columns={\"gold_label\": \"labels\", \"Sentence1\": \"premise\", \"Sentence2\":\"hypothesis\", \"Explanation\": \"explanations\"})\n",
    "    df['labels'] = df['labels'].map({'contradiction': 0, 'neutral': 1, 'entailment': 2})\n",
    "    df[['premise', 'hypothesis','explanations']] = df[['premise', 'hypothesis','explanations']].progress_apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    return df[[\"groups\", \"premise\", \"hypothesis\", \"explanations\", \"labels\", \"explanation_no\"]]\n",
    "    \n",
    "esnli_train_rf = reformat_esnli(esnli_train)\n",
    "esnli_dev_rf = reformat_esnli(esnli_dev)\n",
    "esnli_test_rf = reformat_esnli(esnli_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"ECQA: {ecqa_train.head()}\\ne-SNLI: {esnli_train.head()}\\nComVE: {comve_train.head()}\")\n",
    "print(f\"ECQA: {ecqa_train_rf.head()}\\ne-SNLI: {esnli_train_rf.head()}\\nComVE: {comve_train_rf.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f211bc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"ECQA:\")\n",
    "print(f\"Training length: \\t\\t{str(len(ecqa_train_rf))} \\nValue counts: \\n{ecqa_train_rf['labels'].value_counts()}\\n\" + \n",
    "    f\"Evaluation length: \\t\\t{str(len(ecqa_dev_rf))} \\nValue counts: \\n{ecqa_dev_rf['labels'].value_counts()}\\n\" +\n",
    "     f\"Test length (explantions): \\t{str(len(ecqa_test_rf))} \\nValue counts: \\n{ecqa_test_rf['labels'].value_counts()}\")\n",
    "\n",
    "print(\"\\ne-SNLI:\")\n",
    "print(f\"Training length: \\t\\t{str(len(esnli_train_rf))} \\nValue counts: \\n{esnli_train_rf['labels'].value_counts()}\\n\" + \n",
    "    f\"Evaluation length: \\t\\t{str(len(esnli_dev_rf))} \\nValue counts: \\n{esnli_dev_rf['labels'].value_counts()}\\n\" +\n",
    "     f\"Test length (explantions): \\t{str(len(esnli_test_rf))} \\nValue counts: \\n{esnli_test_rf['labels'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00d90f",
   "metadata": {},
   "source": [
    "Tokenize the preprocessed data into bert format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaece30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast = True)\n",
    "\n",
    "# Tokenizer for a bert input without explanations, for comparison\n",
    "def tokenize_ne(entry):\n",
    "    #<[CLS] <Question> [SEP] <Option> [SEP]\n",
    "    try:\n",
    "        return tokenizer(entry[sen1], entry[sen2])\n",
    "    except:\n",
    "        print(f\"sen1: {entry[sen1]}, sen2: {entry[sen2]}\")\n",
    "# Tokenizer for a bert input\n",
    "def tokenize(entry):\n",
    "    #<CLS> <Question> [SEP] <Option> [SEP] <Explanation> [SEP]\n",
    "    try:\n",
    "        #return tokenizer(entry[sen1] + \" \" + entry[sen2], entry[exp])\n",
    "        return tokenizer(entry[sen1], entry[sen2] + \" [SEP] \" + entry[exp])\n",
    "    except Exception as err: \n",
    "        print(f\"sen1: {entry[sen1]}, sen2: {entry[sen2]}, exp: {entry[exp]}\")\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data in a bert format\n",
    "tqdm.pandas()\n",
    "sen1 = \"questions\"\n",
    "sen2 = \"options\"\n",
    "exp = \"explanations\"\n",
    "bert_ecqa_train_ne = pd.DataFrame(list(ecqa_train_rf.groupby([sen1, sen2]).first().reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "bert_ecqa_dev_ne = pd.DataFrame(list(ecqa_dev_rf.groupby([sen1, sen2]).first().reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "bert_ecqa_test_ne = pd.DataFrame(list(ecqa_test_rf.groupby([sen1, sen2]).first().reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "bert_ecqa_train = pd.DataFrame(list(ecqa_train_rf[ecqa_train_rf['drop_exp'] == False].reset_index().progress_apply(tokenize, axis=1)))\n",
    "bert_ecqa_dev = pd.DataFrame(list(ecqa_dev_rf[ecqa_dev_rf['drop_exp'] == False].reset_index().progress_apply(tokenize, axis=1)))\n",
    "bert_ecqa_test = pd.DataFrame(list(ecqa_test_rf[ecqa_test_rf['drop_exp'] == False].reset_index().progress_apply(tokenize, axis=1)))\n",
    "\n",
    "# Add the labels we lost\n",
    "bert_ecqa_train_ne[['groups', 'questions', 'options', 'labels']] = ecqa_train_rf.groupby([sen1, sen2]).first().reset_index()[['groups', 'questions', 'options', 'labels']]\n",
    "bert_ecqa_dev_ne[['groups', 'questions', 'options', 'labels']] = ecqa_dev_rf.groupby([sen1, sen2]).first().reset_index()[['groups', 'questions', 'options', 'labels']]\n",
    "bert_ecqa_test_ne[['groups', 'questions', 'options', 'labels']] = ecqa_test_rf.groupby([sen1, sen2]).first().reset_index()[['groups', 'questions', 'options', 'labels']]\n",
    "bert_ecqa_train[['groups', 'questions', 'options', 'explanations', 'labels']] = ecqa_train_rf[ecqa_train_rf['drop_exp'] == False].reset_index()[['groups', 'questions', 'options', 'explanations', 'labels']]\n",
    "bert_ecqa_dev[['groups', 'questions', 'options', 'explanations', 'labels']] = ecqa_dev_rf[ecqa_dev_rf['drop_exp'] == False].reset_index()[['groups', 'questions', 'options', 'explanations', 'labels']]\n",
    "bert_ecqa_test[['groups', 'questions', 'options', 'explanations', 'labels']] = ecqa_test_rf[ecqa_test_rf['drop_exp'] == False].reset_index()[['groups', 'questions', 'options', 'explanations', 'labels']]\n",
    "\n",
    "# Save the tokenized bert data\n",
    "bert_ecqa_train.to_csv('tokenized/bert/ecqa/train.csv')\n",
    "bert_ecqa_dev.to_csv('tokenized/bert/ecqa/dev.csv')\n",
    "bert_ecqa_test.to_csv('tokenized/bert/ecqa/test.csv')\n",
    "bert_ecqa_train_ne.to_csv('tokenized/bert/ecqa/train_ne.csv')\n",
    "bert_ecqa_dev_ne.to_csv('tokenized/bert/ecqa/dev_ne.csv')\n",
    "bert_ecqa_test_ne.to_csv('tokenized/bert/ecqa/test_ne.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = \"premise\"\n",
    "sen2 = \"hypothesis\"\n",
    "exp = \"explanations\"\n",
    "\n",
    "bert_esnli_train_ne = pd.DataFrame(list(esnli_train_rf.dropna().groupby([sen1, sen2]).first().reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "bert_esnli_dev_ne = pd.DataFrame(list(esnli_dev_rf.dropna().groupby([sen1, sen2]).first().reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "bert_esnli_test_ne = pd.DataFrame(list(esnli_test_rf.groupby([sen1, sen2]).first().reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "bert_esnli_train = pd.DataFrame(list(esnli_train_rf.dropna().reset_index().progress_apply(tokenize, axis=1)))\n",
    "bert_esnli_dev = pd.DataFrame(list(esnli_dev_rf.dropna().reset_index().progress_apply(tokenize, axis=1)))\n",
    "bert_esnli_test = pd.DataFrame(list(esnli_test_rf.reset_index().progress_apply(tokenize, axis=1)))\n",
    "\n",
    "# Add the labels we lost\n",
    "bert_esnli_train_ne[['groups', 'premise', 'hypothesis', 'labels']] = esnli_train_rf.dropna().groupby([sen1, sen2]).first().reset_index()[['groups', 'premise', 'hypothesis', 'labels']]\n",
    "bert_esnli_dev_ne[['groups', 'premise', 'hypothesis', 'labels']] = esnli_dev_rf.dropna().groupby([sen1, sen2]).first().reset_index()[['groups', 'premise', 'hypothesis', 'labels']]\n",
    "bert_esnli_test_ne[['groups', 'premise', 'hypothesis', 'labels']] = esnli_test_rf.groupby([sen1, sen2]).first().reset_index()[['groups', 'premise', 'hypothesis', 'labels']]\n",
    "bert_esnli_train[['groups', 'premise', 'hypothesis', 'explanations', 'labels']] = esnli_train_rf.dropna().reset_index()[['groups', 'premise', 'hypothesis', 'explanations', 'labels']]\n",
    "bert_esnli_dev[['groups', 'premise', 'hypothesis', 'explanations', 'labels']] = esnli_dev_rf.dropna().reset_index()[['groups', 'premise', 'hypothesis', 'explanations', 'labels']]\n",
    "bert_esnli_test[['groups', 'premise', 'hypothesis', 'explanations', 'labels']] = esnli_test_rf.reset_index()[['groups', 'premise', 'hypothesis', 'explanations', 'labels']]\n",
    "\n",
    "bert_esnli_train.to_csv('tokenized/bert/esnli/train.csv')\n",
    "bert_esnli_dev.to_csv('tokenized/bert/esnli/dev.csv')\n",
    "bert_esnli_test.to_csv('tokenized/bert/esnli/test.csv')\n",
    "bert_esnli_train_ne.to_csv('tokenized/bert/esnli/train_ne.csv')\n",
    "bert_esnli_dev_ne.to_csv('tokenized/bert/esnli/dev_ne.csv')\n",
    "bert_esnli_test_ne.to_csv('tokenized/bert/esnli/test_ne.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54dc6e9",
   "metadata": {},
   "source": [
    "Tokenize the preprocessed data into GPT format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7e057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "#tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenizer for a bert input without explanations, for comparison\n",
    "def tokenize_ne(entry):\n",
    "    try:\n",
    "        return tokenizer(\"Statement: \" + entry[sen1] + \"\\n\" + \n",
    "                         \"Statement: \" + entry[sen2] + \"\\n\" +\n",
    "                         \"Explanation:\")\n",
    "    except Exception as err:\n",
    "        print(\"Statement: \" + entry[sen1] + \"\\n\" + \n",
    "              \"Statement: \" + entry[sen2] + \"\\n\" +\n",
    "              \"Explanation:\")\n",
    "        raise err\n",
    "\n",
    "# Tokenizer for a bert input\n",
    "def tokenize(entry):\n",
    "    try:\n",
    "        return tokenizer(\"Statement: \" + entry[sen1] + \"\\n\" + \n",
    "                         \"Statement: \" + entry[sen2] + \"\\n\" +\n",
    "                         \"Explanation: \" + entry[exp] + tokenizer.eos_token)\n",
    "    except Exception as err: \n",
    "        print(\"Statement: \" + entry[sen1] + \"\\n\" + \n",
    "              \"Statement: \" + entry[sen2] + \"\\n\" +\n",
    "              \"Explanation: \" + entry[exp] + tokenizer.eos_token )\n",
    "        raise err\n",
    "\n",
    "def stokenize_ne(entry):\n",
    "    try:\n",
    "        return tokenizer(\"Statement: \" + entry[sen1] + \"\\n\" + \n",
    "                         \"Explanation:\")\n",
    "    except Exception as err:\n",
    "        print(\"Statement: \" + entry[sen1] + \"\\n\" + \n",
    "              \"Explanation:\")\n",
    "        raise err\n",
    "\n",
    "# Tokenizer for a bert input\n",
    "def stokenize(entry):\n",
    "    try:\n",
    "        return tokenizer(\"Statement: \" + entry[sen1] + \"\\n\" + \n",
    "                         \"Explanation: \" + entry[exp] + tokenizer.eos_token)\n",
    "    except Exception as err: \n",
    "        print(\"Statement: \" + entry[sen1] + \"\\n\" + \n",
    "              \"Explanation: \" + entry[exp] + tokenizer.eos_token )\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data in a GPT-2 format\n",
    "tqdm.pandas()\n",
    "sen1 = \"questions\"\n",
    "sen2 = \"options\"\n",
    "exp = \"explanations\"\n",
    "gpt_ecqa_train_ne = pd.DataFrame(list(ecqa_train_rf[ecqa_train_rf['drop_exp'] == False].dropna().reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "gpt_ecqa_dev_ne = pd.DataFrame(list(ecqa_dev_rf[ecqa_dev_rf['drop_exp'] == False].dropna().reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "gpt_ecqa_test_ne = pd.DataFrame(list(ecqa_test_rf[ecqa_test_rf['drop_exp'] == False].reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "\n",
    "gpt_ecqa_train = pd.DataFrame(list(ecqa_train_rf[ecqa_train_rf['drop_exp'] == False].dropna().reset_index().progress_apply(tokenize, axis=1)))\n",
    "gpt_ecqa_dev = pd.DataFrame(list(ecqa_dev_rf[ecqa_dev_rf['drop_exp'] == False].dropna().reset_index().progress_apply(tokenize, axis=1)))\n",
    "gpt_ecqa_test = pd.DataFrame(list(ecqa_test_rf[ecqa_test_rf['drop_exp'] == False].reset_index().progress_apply(tokenize, axis=1)))\n",
    "gpt_ecqa_final_test = pd.DataFrame(list(ecqa_test_rf.groupby([sen1, sen2]).first().reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "\n",
    "gpt_ecqa_train_ne[['gold_ids', 'gold_mask']] = gpt_ecqa_train[['input_ids', 'attention_mask']]\n",
    "gpt_ecqa_dev_ne[['gold_ids', 'gold_mask']] = gpt_ecqa_dev[['input_ids', 'attention_mask']]\n",
    "gpt_ecqa_test_ne[['gold_ids', 'gold_mask']] = gpt_ecqa_test[['input_ids', 'attention_mask']]\n",
    "\n",
    "gpt_ecqa_train_ne[['groups', 'questions', 'options', 'explanations', 'labels']] = ecqa_train_rf[ecqa_train_rf['drop_exp'] == False].dropna().reset_index()[['groups', 'questions', 'options', 'explanations', 'labels']]\n",
    "gpt_ecqa_dev_ne[['groups', 'questions', 'options', 'explanations', 'labels']] = ecqa_dev_rf[ecqa_dev_rf['drop_exp'] == False].dropna().reset_index()[['groups', 'questions', 'options', 'explanations', 'labels']]\n",
    "gpt_ecqa_test_ne[['groups', 'questions', 'options', 'explanations', 'labels']] = ecqa_test_rf[ecqa_test_rf['drop_exp'] == False].dropna().reset_index()[['groups', 'questions', 'options', 'explanations', 'labels']]\n",
    "gpt_ecqa_final_test[['groups', 'questions', 'options', 'labels']] = ecqa_test_rf.groupby([sen1, sen2]).first().reset_index()[['groups', 'questions', 'options', 'labels']]\n",
    "\n",
    "gpt_ecqa_train_ne.to_csv('tokenized/gpt2/ecqa/train.csv')\n",
    "gpt_ecqa_dev_ne.to_csv('tokenized/gpt2/ecqa/dev.csv')\n",
    "gpt_ecqa_test_ne.to_csv('tokenized/gpt2/ecqa/test.csv')\n",
    "gpt_ecqa_final_test.to_csv('tokenized/gpt2/ecqa/final_test.csv')\n",
    "\n",
    "sen1 = \"premise\"\n",
    "sen2 = \"hypothesis\"\n",
    "exp = \"explanations\"\n",
    "gpt_esnli_train_ne = pd.DataFrame(list(esnli_train_rf.dropna().reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "gpt_esnli_dev_ne = pd.DataFrame(list(esnli_dev_rf.dropna().reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "gpt_esnli_test_ne = pd.DataFrame(list(esnli_test_rf.reset_index().progress_apply(tokenize_ne, axis=1)))\n",
    "\n",
    "gpt_esnli_train = pd.DataFrame(list(esnli_train_rf.dropna().reset_index().progress_apply(tokenize, axis=1)))\n",
    "gpt_esnli_dev = pd.DataFrame(list(esnli_dev_rf.dropna().reset_index().progress_apply(tokenize, axis=1)))\n",
    "gpt_esnli_test = pd.DataFrame(list(esnli_test_rf.reset_index().progress_apply(tokenize, axis=1)))\n",
    "\n",
    "gpt_esnli_train_ne[['gold_ids', 'gold_mask']] = gpt_esnli_train[['input_ids', 'attention_mask']]\n",
    "gpt_esnli_dev_ne[['gold_ids', 'gold_mask']] = gpt_esnli_dev[['input_ids', 'attention_mask']]\n",
    "gpt_esnli_test_ne[['gold_ids', 'gold_mask']] = gpt_esnli_test[['input_ids', 'attention_mask']]\n",
    "\n",
    "gpt_esnli_train_ne[['groups', 'premise', 'hypothesis', 'explanations', 'labels']] = esnli_train_rf.dropna().reset_index()[['groups', 'premise', 'hypothesis', 'explanations', 'labels']]\n",
    "gpt_esnli_dev_ne[['groups', 'premise', 'hypothesis', 'explanations', 'labels']] = esnli_dev_rf.dropna().reset_index()[['groups', 'premise', 'hypothesis', 'explanations', 'labels']]\n",
    "gpt_esnli_test_ne[['groups', 'premise', 'hypothesis', 'explanations', 'labels']] = esnli_test_rf.reset_index()[['groups', 'premise', 'hypothesis', 'explanations', 'labels']]\n",
    "\n",
    "gpt_esnli_train_ne.to_csv('tokenized/gpt2/esnli/train.csv')\n",
    "gpt_esnli_dev_ne.to_csv('tokenized/gpt2/esnli/dev.csv')\n",
    "gpt_esnli_test_ne.to_csv('tokenized/gpt2/esnli/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('generated/multitask/separate/ecqa_train.json', 'r') as f:\n",
    "    ecqa_generated = pd.DataFrame(json.load(f))\n",
    "with open('generated/multitask/separate/comve_train.json', 'r') as f:\n",
    "    esnli_generated = pd.DataFrame(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('generated/multitask/separate/ecqa_test.json', 'r') as f:\n",
    "    ecqa_generated = pd.DataFrame(json.load(f))\n",
    "\n",
    "# Tokenize the data in a bert format\n",
    "tqdm.pandas()\n",
    "sen1 = \"questions\"\n",
    "sen2 = \"options\"\n",
    "exp = \"generated\"\n",
    "ecqa_test = pd.DataFrame(list(ecqa_generated.reset_index().progress_apply(tokenize, axis=1)))\n",
    "ecqa_test[['groups', 'data_id', 'questions', 'options', 'explanations', 'generated', 'labels']] = ecqa_generated.reset_index()[['groups', 'data_id', 'questions', 'options', 'explanations', 'generated', 'labels']]\n",
    "ecqa_test.to_csv('generated/multitask/separate/ecqa_test_gpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23544790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "with open('generated/multitask/separate/esnli_test.json', 'r') as f:\n",
    "    esnli_generated = pd.DataFrame(json.load(f))\n",
    "\n",
    "# Tokenize the data in a bert format\n",
    "tqdm.pandas()\n",
    "sen1 = \"premise\"\n",
    "sen2 = \"hypothesis\"\n",
    "exp = \"generated\"\n",
    "esnli_test = pd.DataFrame(list(esnli_generated.reset_index().progress_apply(tokenize, axis=1)))\n",
    "esnli_test[['groups', 'premise', 'hypothesis', 'generated', 'labels']] = esnli_generated.reset_index()[['groups', 'premise', 'hypothesis', 'generated', 'labels']]\n",
    "esnli_test.to_csv('generated/multitask/separate/esnli_test_gpt.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
